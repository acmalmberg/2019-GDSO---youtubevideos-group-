{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from glob import glob\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(record, training=True):\n",
    "    \"\"\"\n",
    "    In training mode labels will be returned, otherwise they won't be\n",
    "    \"\"\"\n",
    "    keys_to_features = {\n",
    "        \"mean_rgb\": tf.FixedLenFeature([1024], tf.float32),\n",
    "        \"mean_audio\": tf.FixedLenFeature([128], tf.float32)\n",
    "    }\n",
    "    \n",
    "    if training:\n",
    "        keys_to_features[\"labels\"] =  tf.VarLenFeature(tf.int64)\n",
    "    \n",
    "    parsed = tf.parse_single_example(record, keys_to_features)\n",
    "    x = tf.concat([parsed[\"mean_rgb\"], parsed[\"mean_audio\"]], axis=0)\n",
    "    if training:\n",
    "        y = tf.sparse_to_dense(parsed[\"labels\"].values, [3862], 1)\n",
    "        return x, y\n",
    "    else:\n",
    "        x = tf.concat([parsed[\"mean_rgb\"], parsed[\"mean_audio\"]], axis=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_datasetprovider(tf_records, repeats=1000, num_parallel_calls=12, \n",
    "                         batch_size=32): \n",
    "    \"\"\"\n",
    "    tf_records: list of strings - tf records you are going to use.\n",
    "    repeats: how many times you want to iterate over the data.\n",
    "    \"\"\"\n",
    "    dataset = tf.data.TFRecordDataset(tf_records)\n",
    "    dataset = dataset.map(map_func=parser, num_parallel_calls=num_parallel_calls)\n",
    "    dataset = dataset.repeat(repeats)\n",
    "\n",
    "    dataset = dataset.shuffle(buffer_size=1000)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    d_iter = dataset.make_one_shot_iterator()\n",
    "    return d_iter\n",
    "\n",
    "def data_generator(tf_records, batch_size=1, repeats=1000, num_parallel_calls=12, ):\n",
    "    \n",
    "    tf_provider = make_datasetprovider(tf_records, repeats=repeats, num_parallel_calls=num_parallel_calls,\n",
    "                                       batch_size=batch_size)\n",
    "    sess = tf.Session()\n",
    "    next_el = tf_provider.get_next()\n",
    "    while True:\n",
    "        try:\n",
    "            yield sess.run(next_el)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"Iterations exhausted\")\n",
    "            break\n",
    "            \n",
    "def fetch_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(2048, activation=\"relu\", input_shape=(1024 + 128,)))\n",
    "    model.add(Dense(3862, activation=\"sigmoid\"))\n",
    "    model.compile(\"adam\", loss=\"binary_crossentropy\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = glob(\"Train/train0093.tfrecord\")\n",
    "eval_data = glob(\"Validate/validate0024.tfrecord\")\n",
    "\n",
    "my_train_iter = data_generator(train_data)\n",
    "my_eval_iter = data_generator(eval_data)\n",
    "model = fetch_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "30/30 [==============================] - 7s 242ms/step - loss: 0.1732 - val_loss: 0.0495\n",
      "Epoch 2/10\n",
      "30/30 [==============================] - 4s 146ms/step - loss: 0.0266 - val_loss: 0.0340\n",
      "Epoch 3/10\n",
      "30/30 [==============================] - 4s 146ms/step - loss: 0.0187 - val_loss: 0.0102\n",
      "Epoch 4/10\n",
      "30/30 [==============================] - 4s 137ms/step - loss: 0.0103 - val_loss: 0.0085\n",
      "Epoch 5/10\n",
      "30/30 [==============================] - 4s 145ms/step - loss: 0.0067 - val_loss: 0.0077\n",
      "Epoch 6/10\n",
      "30/30 [==============================] - 4s 138ms/step - loss: 0.0097 - val_loss: 0.0083\n",
      "Epoch 7/10\n",
      "30/30 [==============================] - 4s 138ms/step - loss: 0.0065 - val_loss: 0.0074\n",
      "Epoch 8/10\n",
      "30/30 [==============================] - 4s 139ms/step - loss: 0.0068 - val_loss: 0.0062\n",
      "Epoch 9/10\n",
      "30/30 [==============================] - 4s 138ms/step - loss: 0.0057 - val_loss: 0.0061\n",
      "Epoch 10/10\n",
      "30/30 [==============================] - 4s 140ms/step - loss: 0.0070 - val_loss: 0.0060\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb302236d8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(my_train_iter, \n",
    "                    steps_per_epoch=30, \n",
    "                    epochs=10, \n",
    "                    validation_data=my_eval_iter, \n",
    "                    validation_steps=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
